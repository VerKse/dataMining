1. загружаем датасет и берём из него сочетания признаков + следствия из них;

2. чтобы избежать переобучения — разбиваем набор на две части, на train применяем классификатор и лучшей модель применяем к test;

3. в gridSearch меняя гиперпараметры, оцениваем f-меру и сравниваем сначала с baseLine, потом между собой;

4. folds (https://scikit-learn.org/stable/modules/cross_validation.html)
В базовом подходе, называемом k-кратным CV, обучающий набор разбивается на k меньших наборов. Для каждого набора исполняется следующее:
Модель обучается с использованием k-1 складок в качестве обучающих данных; Полученная модель проверяется на оставшейся части данных (т.е. она используется в качестве тестового набора для вычисления показателя производительности, такого как точность).

5. f-мера vs accuracy
accuracy = P/N, где P  – количество правильных решений, N – размер обучающей выборки.
f - производная метрика, базисом которой являются точность (precision) и полнота (recall) 
точность -- частное истинно-положительных решений ко всем положительным (в т.ч. ложно-положительным)
полнота -- частное истинно-положительных решений к сумме истинно-положительных и ложно-отрицательных
f-мера = гармоническое среднее между точностью и полнотой (стремится к нулю если точность или плотность равна нулю)
F=2(Precision×Recall)/(Precision+Recall)
Усовершенствованная формула предполагает придание преобладающего веса точности или полноте

/*Классификаторы*/
6. LogisticRegression -- метод сравнения с предполагемой криваой,позволяет оценивать вероятности принадлежености классам

7. SVM (метод опорных векторов) -- сравнение объектов относительно прямых, разделающих классы
Метод опорных векторов максимизирует отступы объектов, что тесно связано с минимизацией вероятности переобучения. При этом он позволяет очень легко перейти к построению нелинейной разделяющей поверхности (объёмной) при помощи ядрового перехода. 

8. DecisionTreeClassifier (деревья решений) -- cтруктура дерева представляет собой «листья» и «ветки». На ветках записаны атрибуты, от которых зависит целевая функция. Значения целевой функции записаны в «листьях», а в остальных узлах — атрибуты, по которым различаются случаи. Чтобы классифицировать новый случай, надо спуститься по дереву до листа и выдать соответствующее значение.
min_samples_leaf = точка разделения на любой глубине будет учитываться только в том случае, если она оставляет не менее min_samples_leaf обучающих выборок в каждой из левой и правой ветвей. Это может иметь эффект сглаживания модели, особенно в регрессии.

9. NaiveBayes (наивный байесовский) -- крайне простой, при этом часто работает лучше всех даже в сложных ситуациях. Основан (шок) на теореме Байеса о вероятности наступления гипотезы A при событии B 

10. RandomForestClassifier (случайный лес) -- ансамбль решающих деревьев, качество за счёт количества

11. AdaBoostClassifier (адаптивный бустинг) -- каждый следующий классификатор строится по объектам, которые плохо классифицируются предыдущими классификаторами
n_estimators = максимальное количество оценок, при которых бустинг прекращается. В случае идеальной подгонки процедура обучения прекращается рано.
learning_rate = сокращает вклад каждого классификатора

12. VotingClassifier -- модуль использования сразу нескольких, не похожих между собой, моделей машинного обучения и их объединения в один классификатор
voting = «hard», использует прогнозируемые метки классов для голосования по правилу большинства. «soft», прогнозирует метку класса на основе argmax сумм прогнозируемых вероятностей

