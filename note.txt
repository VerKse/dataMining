1. загружаем датасет и берём из него сочетания признаков + следствия из них;

2. чтобы избежать переобучения — разбиваем набор на две части, на train применяем классификатор и лучшей модель применяем к test;

3. в gridSearch меняя гиперпараметры, оцениваем f-меру и сравниваем сначала с baseLine, потом между собой;

4. folds (https://scikit-learn.org/stable/modules/cross_validation.html)
В базовом подходе, называемом k-кратным CV, обучающий набор разбивается на k меньших наборов. Для каждого набора исполняется следующее:
Модель обучается с использованием k-1 складок в качестве обучающих данных; Полученная модель проверяется на оставшейся части данных (т.е. она используется в качестве тестового набора для вычисления показателя производительности, такого как точность).

5. f-мера vs accuracy
accuracy = P/N, где P  – количество правильных решений, N – размер обучающей выборки.
f - производная метрика, базисом которой являются точность (precision) и полнота (recall) 
точность -- частное истинно-положительных решений ко всем положительным (в т.ч. ложно-положительным)
полнота -- частное истинно-положительных решений к сумме истинно-положительных и ложно-отрицательных
f-мера = гармоническое среднее между точностью и полнотой (стремится к нулю если точность или плотность равна нулю)
F=2(Precision×Recall)/(Precision+Recall)
Усовершенствованная формула предполагает придание преобладающего веса точности или полноте

6. LogisticRegression -- метод сравнения с предполагемой кривой

7. SVM (метод опорных векторов) -- сравнение объектов относительно прямых, разделающих классы

8. DecisionTreeClassifier (деревья решений) -- cтруктура дерева представляет собой «листья» и «ветки». На ветках записаны атрибуты, от которых зависит целевая функция. Значения целевой функции записаны в «листьях», а в остальных узлах — атрибуты, по которым различаются случаи. Чтобы классифицировать новый случай, надо спуститься по дереву до листа и выдать соответствующее значение.

9. NaiveBayes (наивный байесовский) -- крайне простой, при этом часто работает лучше всех даже в сложных ситуациях. Основан (шок) на теореме Байеса о вероятности наступления гипотезы A при событии B 

10. RandomForestClassifier (случайный лес) -- ансамбль решающих деревьев, качество за счёт количества

11. AdaBoostClassifier (адаптивный бустинг)

12. VotingClassifier



